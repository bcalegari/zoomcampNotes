{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52bad16a",
   "metadata": {},
   "source": [
    "# Data loading \n",
    "\n",
    "Here we will be using the ```.paraquet``` file we downloaded and do the following:\n",
    " - Check metadata and table datatypes of the paraquet file/table\n",
    " - Convert the paraquet file to pandas dataframe and check the datatypes. Additionally check the data dictionary to make sure you have the right datatypes in pandas, as pandas will automatically create the table in our database.\n",
    " - Generate the DDL CREATE statement from pandas for a sanity check.\n",
    " - Create a connection to our database using SQLAlchemy\n",
    " - Convert our huge paraquet file into a iterable that has batches of 100,000 rows and load it into our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afef2456",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T23:55:14.141738Z",
     "start_time": "2023-12-03T23:55:14.124217Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pyarrow.parquet as pq\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c750d1d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T02:54:01.925350Z",
     "start_time": "2023-12-03T02:54:01.661119Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x74dfc27c51c0>\n",
       "  created_by: parquet-cpp-arrow version 7.0.0\n",
       "  num_columns: 19\n",
       "  num_rows: 1369769\n",
       "  num_row_groups: 1\n",
       "  format_version: 1.0\n",
       "  serialized_size: 10382"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read metadata \n",
    "pq.read_metadata('../resources/yellow_tripdata_2021-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a970fcf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T23:28:08.411945Z",
     "start_time": "2023-12-03T23:28:08.177693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VendorID: int64\n",
       "tpep_pickup_datetime: timestamp[us]\n",
       "tpep_dropoff_datetime: timestamp[us]\n",
       "passenger_count: double\n",
       "trip_distance: double\n",
       "RatecodeID: double\n",
       "store_and_fwd_flag: string\n",
       "PULocationID: int64\n",
       "DOLocationID: int64\n",
       "payment_type: int64\n",
       "fare_amount: double\n",
       "extra: double\n",
       "mta_tax: double\n",
       "tip_amount: double\n",
       "tolls_amount: double\n",
       "improvement_surcharge: double\n",
       "total_amount: double\n",
       "congestion_surcharge: double\n",
       "airport_fee: double\n",
       "-- schema metadata --\n",
       "pandas: '{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":' + 2492"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file, read the table from file and check schema\n",
    "file = pq.ParquetFile('../resources/yellow_tripdata_2021-01.parquet')\n",
    "table = file.read()\n",
    "table.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43f6ea7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T23:28:22.870376Z",
     "start_time": "2023-12-03T23:28:22.563414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1369769 entries, 0 to 1369768\n",
      "Data columns (total 19 columns):\n",
      " #   Column                 Non-Null Count    Dtype         \n",
      "---  ------                 --------------    -----         \n",
      " 0   VendorID               1369769 non-null  int64         \n",
      " 1   tpep_pickup_datetime   1369769 non-null  datetime64[us]\n",
      " 2   tpep_dropoff_datetime  1369769 non-null  datetime64[us]\n",
      " 3   passenger_count        1271417 non-null  float64       \n",
      " 4   trip_distance          1369769 non-null  float64       \n",
      " 5   RatecodeID             1271417 non-null  float64       \n",
      " 6   store_and_fwd_flag     1271417 non-null  object        \n",
      " 7   PULocationID           1369769 non-null  int64         \n",
      " 8   DOLocationID           1369769 non-null  int64         \n",
      " 9   payment_type           1369769 non-null  int64         \n",
      " 10  fare_amount            1369769 non-null  float64       \n",
      " 11  extra                  1369769 non-null  float64       \n",
      " 12  mta_tax                1369769 non-null  float64       \n",
      " 13  tip_amount             1369769 non-null  float64       \n",
      " 14  tolls_amount           1369769 non-null  float64       \n",
      " 15  improvement_surcharge  1369769 non-null  float64       \n",
      " 16  total_amount           1369769 non-null  float64       \n",
      " 17  congestion_surcharge   1271417 non-null  float64       \n",
      " 18  airport_fee            5 non-null        float64       \n",
      "dtypes: datetime64[us](2), float64(12), int64(4), object(1)\n",
      "memory usage: 198.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Convert to pandas and check data \n",
    "df = table.to_pandas()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf039a0",
   "metadata": {},
   "source": [
    "We need to first create the connection to our postgres database. We can feed the connection information to generate the CREATE SQL query for the specific server. SQLAlchemy supports a variety of servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44e701ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T22:50:25.811951Z",
     "start_time": "2023-12-03T22:50:25.393987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x74dfc1d2ff50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an open SQL database connection object or a SQLAlchemy connectable\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n",
    "engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c96a1075",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T22:50:43.628727Z",
     "start_time": "2023-12-03T22:50:43.442337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE yellow_taxi_data (\n",
      "\t\"VendorID\" BIGINT, \n",
      "\ttpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\ttpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tpassenger_count FLOAT(53), \n",
      "\ttrip_distance FLOAT(53), \n",
      "\t\"RatecodeID\" FLOAT(53), \n",
      "\tstore_and_fwd_flag TEXT, \n",
      "\t\"PULocationID\" BIGINT, \n",
      "\t\"DOLocationID\" BIGINT, \n",
      "\tpayment_type BIGINT, \n",
      "\tfare_amount FLOAT(53), \n",
      "\textra FLOAT(53), \n",
      "\tmta_tax FLOAT(53), \n",
      "\ttip_amount FLOAT(53), \n",
      "\ttolls_amount FLOAT(53), \n",
      "\timprovement_surcharge FLOAT(53), \n",
      "\ttotal_amount FLOAT(53), \n",
      "\tcongestion_surcharge FLOAT(53), \n",
      "\tairport_fee FLOAT(53)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate CREATE SQL statement from schema for validation\n",
    "print(pd.io.sql.get_schema(df, name='yellow_taxi_data', con=engine))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca7f32d",
   "metadata": {},
   "source": [
    "Datatypes for the table looks good! Since we used paraquet file the datasets seem to have been preserved. You may have to convert some datatypes so it is always good to do this check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a751ed",
   "metadata": {},
   "source": [
    "## Finally inserting data\n",
    "\n",
    "There are 2,846,722 rows in our dataset. We are going to use the ```parquet_file.iter_batches()``` function to create batches of 100,000, convert them into pandas and then load it into the postgres database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20cec73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T23:49:28.768786Z",
     "start_time": "2023-12-03T23:49:28.689732Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This part is for testing\n",
    "\n",
    "\n",
    "# Creating batches of 100,000 for the paraquet file\n",
    "batches_iter = file.iter_batches(batch_size=100000)\n",
    "batches_iter\n",
    "\n",
    "# Take the first batch for testing\n",
    "df = next(batches_iter).to_pandas()\n",
    "df\n",
    "\n",
    "# Creating just the table in postgres\n",
    "# df.head(0).to_sql(name='yellow_taxi_data',con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fdda025",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T00:08:07.651559Z",
     "start_time": "2023-12-04T00:02:35.940526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserting batch 1...\n",
      "inserted! time taken     15.650 seconds.\n",
      "\n",
      "inserting batch 2...\n",
      "inserted! time taken     13.056 seconds.\n",
      "\n",
      "inserting batch 3...\n",
      "inserted! time taken     13.525 seconds.\n",
      "\n",
      "inserting batch 4...\n",
      "inserted! time taken     12.725 seconds.\n",
      "\n",
      "inserting batch 5...\n",
      "inserted! time taken     12.452 seconds.\n",
      "\n",
      "inserting batch 6...\n",
      "inserted! time taken     12.678 seconds.\n",
      "\n",
      "inserting batch 7...\n",
      "inserted! time taken     12.737 seconds.\n",
      "\n",
      "inserting batch 8...\n",
      "inserted! time taken     12.194 seconds.\n",
      "\n",
      "inserting batch 9...\n",
      "inserted! time taken     13.336 seconds.\n",
      "\n",
      "inserting batch 10...\n",
      "inserted! time taken     12.390 seconds.\n",
      "\n",
      "inserting batch 11...\n",
      "inserted! time taken     14.203 seconds.\n",
      "\n",
      "inserting batch 12...\n",
      "inserted! time taken     14.310 seconds.\n",
      "\n",
      "inserting batch 13...\n",
      "inserted! time taken     15.251 seconds.\n",
      "\n",
      "inserting batch 14...\n",
      "inserted! time taken      9.893 seconds.\n",
      "\n",
      "Completed! Total time taken was    184.721 seconds for 14 batches.\n"
     ]
    }
   ],
   "source": [
    "# Insert values into the table \n",
    "t_start = time()\n",
    "count = 0\n",
    "for batch in file.iter_batches(batch_size=100000):\n",
    "    count+=1\n",
    "    batch_df = batch.to_pandas()\n",
    "    print(f'inserting batch {count}...')\n",
    "    b_start = time()\n",
    "    \n",
    "    batch_df.to_sql(name='yellow_taxi_data',con=engine, if_exists='append')\n",
    "    b_end = time()\n",
    "    print(f'inserted! time taken {b_end-b_start:10.3f} seconds.\\n')\n",
    "    \n",
    "t_end = time()   \n",
    "print(f'Completed! Total time taken was {t_end-t_start:10.3f} seconds for {count} batches.')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c102be",
   "metadata": {},
   "source": [
    "## Extra bit\n",
    "\n",
    "While trying to do the SQL Refresher, there was a need to add a lookup zones table but the file is in ```.csv``` format. \n",
    "\n",
    "Let's code to handle both ```.csv``` and ```.paraquet``` files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a643d171",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:59:29.236458Z",
     "start_time": "2023-12-05T20:59:28.551221Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pandas as pd \n",
    "import pyarrow.parquet as pq\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62c9040a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:18:11.346552Z",
     "start_time": "2023-12-05T21:18:11.337475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yellow_tripdata_2021-01.parquet'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv'\n",
    "url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet'\n",
    "\n",
    "file_name = url.rsplit('/', 1)[-1].strip()\n",
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e495fa96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:18:33.001561Z",
     "start_time": "2023-12-05T21:18:32.844872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file detected.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Failed to open local file 'yellow_tripdata_2021-01.parquet'. Detail: [errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m file_name:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParquet file detected.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     file \u001b[38;5;241m=\u001b[39m pq\u001b[38;5;241m.\u001b[39mParquetFile(file_name)\n\u001b[1;32m      8\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(file\u001b[38;5;241m.\u001b[39miter_batches(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m))\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m      9\u001b[0m     df_iter \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39miter_batches(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pyarrow/parquet/core.py:313\u001b[0m, in \u001b[0;36mParquetFile.__init__\u001b[0;34m(self, source, metadata, common_metadata, read_dictionary, memory_map, buffer_size, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, filesystem, page_checksum_verification)\u001b[0m\n\u001b[1;32m    310\u001b[0m filesystem, source \u001b[38;5;241m=\u001b[39m _resolve_filesystem_and_path(\n\u001b[1;32m    311\u001b[0m     source, filesystem, memory_map\u001b[38;5;241m=\u001b[39mmemory_map)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filesystem \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 313\u001b[0m     source \u001b[38;5;241m=\u001b[39m filesystem\u001b[38;5;241m.\u001b[39mopen_input_file(source)\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# We opened it here, ensure we close it.\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader \u001b[38;5;241m=\u001b[39m ParquetReader()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pyarrow/_fs.pyx:783\u001b[0m, in \u001b[0;36mpyarrow._fs.FileSystem.open_input_file\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pyarrow/error.pxi:154\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pyarrow/error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Failed to open local file 'yellow_tripdata_2021-01.parquet'. Detail: [errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "if '.csv' in file_name:\n",
    "    print('CSV file detected.') \n",
    "    df = pd.read_csv(file_name, nrows=10)\n",
    "    df_iter = pd.read_csv(file_name, iterator=True, chunksize=100000)\n",
    "elif '.parquet' in file_name:\n",
    "    print('Parquet file detected.')\n",
    "    file = pq.ParquetFile(file_name)\n",
    "    df = next(file.iter_batches(batch_size=10)).to_pandas()\n",
    "    df_iter = file.iter_batches(batch_size=100000)\n",
    "else: \n",
    "    print('Error. Only .csv or .parquet files allowed.')\n",
    "    sys.exit() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556748f",
   "metadata": {},
   "source": [
    "This code is a rough code and seems to be working. The cleaned up version will be in `data-loading-parquet.py` file."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
